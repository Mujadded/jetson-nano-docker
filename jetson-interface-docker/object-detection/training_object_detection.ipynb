{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c88a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/usr/local/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/torchvision-0.10.0a0+300a8a4-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/Pillow-8.3.1-py3.6-linux-aarch64.egg', '/usr/local/lib/python3.6/dist-packages/torchaudio-0.9.0a0+33b2469-py3.6-linux-aarch64.egg', '/usr/lib/python3/dist-packages', '/usr/lib/python3.6/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/jetson-inference/python/training/detection/ssd']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../jetson-inference/python/training/detection/ssd'))\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec38762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "\n",
    "from vision.utils.misc import Timer, freeze_net_layers, store_labels\n",
    "from vision.ssd.ssd import MatchPrior\n",
    "from vision.ssd.vgg_ssd import create_vgg_ssd\n",
    "from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\n",
    "from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite\n",
    "from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\n",
    "from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite\n",
    "from vision.datasets.voc_dataset import VOCDataset\n",
    "from vision.datasets.open_images import OpenImagesDataset\n",
    "from vision.nn.multibox_loss import MultiboxLoss\n",
    "from vision.ssd.config import vgg_ssd_config\n",
    "from vision.ssd.config import mobilenetv1_ssd_config\n",
    "from vision.ssd.config import squeezenet_ssd_config\n",
    "from vision.ssd.data_preprocessing import TrainAugmentation, TestTransform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a76ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\n",
    "from vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor\n",
    "from vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor\n",
    "from vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor\n",
    "from vision.datasets.voc_dataset import VOCDataset\n",
    "from vision.datasets.open_images import OpenImagesDataset\n",
    "from vision.utils import box_utils, measurements\n",
    "from vision.utils.misc import str2bool, Timer\n",
    "import argparse\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor\n",
    "\n",
    "\n",
    "class MeanAPEvaluator:\n",
    "    \"\"\"\n",
    "    Mean Average Precision (mAP) evaluator\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, net, arch='mb1-ssd', eval_dir='models/eval_results', \n",
    "                 nms_method='hard', iou_threshold=0.5, use_2007_metric=True, device='cuda:0'):\n",
    "                 \n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.use_2007_metric = use_2007_metric\n",
    "\n",
    "        self.eval_path = pathlib.Path(eval_dir)\n",
    "        self.eval_path.mkdir(exist_ok=True)\n",
    "    \n",
    "        self.true_case_stat, self.all_gb_boxes, self.all_difficult_cases = self.group_annotation_by_class(self.dataset)\n",
    "        \n",
    "        if arch == 'vgg16-ssd':\n",
    "            self.predictor = create_vgg_ssd_predictor(net, nms_method=nms_method, device=device)\n",
    "        elif arch == 'mb1-ssd':\n",
    "            self.predictor = create_mobilenetv1_ssd_predictor(net, nms_method=nms_method, device=device)\n",
    "        elif arch == 'mb1-ssd-lite':\n",
    "            self.predictor = create_mobilenetv1_ssd_lite_predictor(net, nms_method=nms_method, device=device)\n",
    "        elif arch == 'sq-ssd-lite':\n",
    "            self.predictor = create_squeezenet_ssd_lite_predictor(net,nms_method=nms_method, device=device)\n",
    "        elif arch == 'mb2-ssd-lite':\n",
    "            self.predictor = create_mobilenetv2_ssd_lite_predictor(net, nms_method=nms_method, device=device)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid network architecture type '{arch}' - it should be one of:  vgg16-ssd, mb1-ssd, mb1-ssd-lite, mb2-ssd-lite, sq-ssd-lite\")\n",
    "\n",
    "    def compute(self):\n",
    "        is_test = self.net.is_test\n",
    "        self.net.is_test = True\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            logging.debug(f\"evaluating average precision   image {i} / {len(self.dataset)}\")\n",
    "            image = self.dataset.get_image(i)\n",
    "            boxes, labels, probs = self.predictor.predict(image)\n",
    "            indexes = torch.ones(labels.size(0), 1, dtype=torch.float32) * i\n",
    "            results.append(torch.cat([\n",
    "                indexes.reshape(-1, 1),\n",
    "                labels.reshape(-1, 1).float(),\n",
    "                probs.reshape(-1, 1),\n",
    "                boxes + 1.0  # matlab's indexes start from 1\n",
    "            ], dim=1))\n",
    "            \n",
    "        results = torch.cat(results)\n",
    "        self.net.is_test = is_test\n",
    "        \n",
    "        for class_index, class_name in enumerate(self.dataset.class_names):\n",
    "            if class_index == 0: continue  # ignore background\n",
    "            prediction_path = self.eval_path / f\"det_test_{class_name}.txt\"\n",
    "            with open(prediction_path, \"w\") as f:\n",
    "                sub = results[results[:, 1] == class_index, :]\n",
    "                for i in range(sub.size(0)):\n",
    "                    prob_box = sub[i, 2:].numpy()\n",
    "                    image_id = self.dataset.ids[int(sub[i, 0])]\n",
    "                    print(\n",
    "                        image_id + \"\\t\" + \" \".join([str(v) for v in prob_box]).replace(\" \", \"\\t\"),\n",
    "                        file=f\n",
    "                    )\n",
    "        aps = []\n",
    "        \n",
    "        for class_index, class_name in enumerate(self.dataset.class_names):\n",
    "            if class_index == 0:\n",
    "                continue\n",
    "            prediction_path = self.eval_path / f\"det_test_{class_name}.txt\"\n",
    "            ap = self.compute_average_precision_per_class(\n",
    "                self.true_case_stat[class_index],\n",
    "                self.all_gb_boxes[class_index],\n",
    "                self.all_difficult_cases[class_index],\n",
    "                prediction_path,\n",
    "                self.iou_threshold,\n",
    "                self.use_2007_metric\n",
    "            )\n",
    "            aps.append(ap)\n",
    "\n",
    "        return sum(aps)/len(aps), aps\n",
    "      \n",
    "    def log_results(self, mean_ap, class_ap, prefix=''):\n",
    "        logging.info(f\"{prefix}Average Precision Per-class:\")\n",
    "        \n",
    "        for i in range(len(class_ap)):\n",
    "            logging.info(f\"    {self.dataset.class_names[i+1]}: {class_ap[i]}\")\n",
    "            \n",
    "        logging.info(f\"{prefix}Mean Average Precision (mAP):  {mean_ap}\")\n",
    "        \n",
    "    def group_annotation_by_class(self, dataset):\n",
    "        true_case_stat = {}\n",
    "        all_gt_boxes = {}\n",
    "        all_difficult_cases = {}\n",
    "        for i in range(len(dataset)):\n",
    "            image_id, annotation = dataset.get_annotation(i)\n",
    "            gt_boxes, classes, is_difficult = annotation\n",
    "            gt_boxes = torch.from_numpy(gt_boxes)\n",
    "            for i, difficult in enumerate(is_difficult):\n",
    "                class_index = int(classes[i])\n",
    "                gt_box = gt_boxes[i]\n",
    "                if not difficult:\n",
    "                    true_case_stat[class_index] = true_case_stat.get(class_index, 0) + 1\n",
    "\n",
    "                if class_index not in all_gt_boxes:\n",
    "                    all_gt_boxes[class_index] = {}\n",
    "                if image_id not in all_gt_boxes[class_index]:\n",
    "                    all_gt_boxes[class_index][image_id] = []\n",
    "                all_gt_boxes[class_index][image_id].append(gt_box)\n",
    "                if class_index not in all_difficult_cases:\n",
    "                    all_difficult_cases[class_index]={}\n",
    "                if image_id not in all_difficult_cases[class_index]:\n",
    "                    all_difficult_cases[class_index][image_id] = []\n",
    "                all_difficult_cases[class_index][image_id].append(difficult)\n",
    "\n",
    "        for class_index in all_gt_boxes:\n",
    "            for image_id in all_gt_boxes[class_index]:\n",
    "                all_gt_boxes[class_index][image_id] = torch.stack(all_gt_boxes[class_index][image_id])\n",
    "        for class_index in all_difficult_cases:\n",
    "            for image_id in all_difficult_cases[class_index]:\n",
    "                all_gt_boxes[class_index][image_id] = all_gt_boxes[class_index][image_id].clone().detach() #torch.tensor(all_gt_boxes[class_index][image_id])\n",
    "        return true_case_stat, all_gt_boxes, all_difficult_cases\n",
    "\n",
    "\n",
    "    def compute_average_precision_per_class(self, num_true_cases, gt_boxes, difficult_cases,\n",
    "                                            prediction_file, iou_threshold, use_2007_metric):\n",
    "        with open(prediction_file) as f:\n",
    "            image_ids = []\n",
    "            boxes = []\n",
    "            scores = []\n",
    "            for line in f:\n",
    "                t = line.rstrip().split(\"\\t\")\n",
    "                image_ids.append(t[0])\n",
    "                scores.append(float(t[1]))\n",
    "                box = torch.tensor([float(v) for v in t[2:]]).unsqueeze(0)\n",
    "                box -= 1.0  # convert to python format where indexes start from 0\n",
    "                boxes.append(box)\n",
    "            scores = np.array(scores)\n",
    "            sorted_indexes = np.argsort(-scores)\n",
    "            boxes = [boxes[i] for i in sorted_indexes]\n",
    "            image_ids = [image_ids[i] for i in sorted_indexes]\n",
    "            true_positive = np.zeros(len(image_ids))\n",
    "            false_positive = np.zeros(len(image_ids))\n",
    "            matched = set()\n",
    "            for i, image_id in enumerate(image_ids):\n",
    "                box = boxes[i]\n",
    "                if image_id not in gt_boxes:\n",
    "                    false_positive[i] = 1\n",
    "                    continue\n",
    "\n",
    "                gt_box = gt_boxes[image_id]\n",
    "                ious = box_utils.iou_of(box, gt_box)\n",
    "                max_iou = torch.max(ious).item()\n",
    "                max_arg = torch.argmax(ious).item()\n",
    "                if max_iou > iou_threshold:\n",
    "                    if difficult_cases[image_id][max_arg] == 0:\n",
    "                        if (image_id, max_arg) not in matched:\n",
    "                            true_positive[i] = 1\n",
    "                            matched.add((image_id, max_arg))\n",
    "                        else:\n",
    "                            false_positive[i] = 1\n",
    "                else:\n",
    "                    false_positive[i] = 1\n",
    "\n",
    "        true_positive = true_positive.cumsum()\n",
    "        false_positive = false_positive.cumsum()\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / num_true_cases\n",
    "        if use_2007_metric:\n",
    "            return measurements.compute_voc2007_average_precision(precision, recall)\n",
    "        else:\n",
    "            return measurements.compute_average_precision(precision, recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aa7be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='/nvdli-nano/data/objectdetection/images/fruit'\n",
    "MODEL_DIR='/nvdli-nano/data/objectdetection/model/fruit'\n",
    "DEFAULT_PRETRAINED_MODEL='../jetson-inference/python/training/detection/ssd/models/mobilenet-v1-ssd-mp-0_675.pth'\n",
    "CHECK_POINT='../jetson-inference/python/training/detection/ssd/models/'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "                \n",
    "logging.basicConfig(stream=sys.stdout, level=getattr(logging, 'INFO', logging.INFO), format='%(asctime)s - %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "                    \n",
    "tensorboard = SummaryWriter(log_dir=os.path.join(CHECK_POINT, \"tensorboard\", f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"))\n",
    "timer = Timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af78b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH = 4\n",
    "DEBUG_STEPS = 10\n",
    "NUM_WORKERS = 2\n",
    "VALIDATION_EPOCHS=1\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "GAMA = 0.1\n",
    "BASE_NET_LR = 0.001\n",
    "T_MAX = 100\n",
    "DEVICE='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa42c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:06:42 - Download https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv.\n",
      "2024-02-16 17:06:43 - Requested 4 classes, found 4 classes\n",
      "2024-02-16 17:06:43 - Download https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv.\n",
      "2024-02-16 17:07:29 - Read annotation file /nvdli-nano/data/objectdetection/images/fruit/train-annotations-bbox.csv\n",
      "2024-02-16 17:09:40 - Available train images:  3683\n",
      "2024-02-16 17:09:40 - Available train boxes:   18935\n",
      "\n",
      "2024-02-16 17:09:40 - Download https://storage.googleapis.com/openimages/2018_04/validation/validation-annotations-bbox.csv.\n",
      "2024-02-16 17:09:41 - Read annotation file /nvdli-nano/data/objectdetection/images/fruit/validation-annotations-bbox.csv\n",
      "2024-02-16 17:09:42 - Available validation images:  186\n",
      "2024-02-16 17:09:42 - Available validation boxes:   598\n",
      "\n",
      "2024-02-16 17:09:42 - Download https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv.\n",
      "2024-02-16 17:09:45 - Read annotation file /nvdli-nano/data/objectdetection/images/fruit/test-annotations-bbox.csv\n",
      "2024-02-16 17:09:48 - Available test images:  631\n",
      "2024-02-16 17:09:48 - Available test boxes:   2041\n",
      "\n",
      "2024-02-16 17:09:48 - Total available images: 4500\n",
      "2024-02-16 17:09:48 - Total available boxes:  21574\n",
      "\n",
      "2024-02-16 17:09:48 - Limiting train dataset to:  2046 images (10602 boxes)\n",
      "2024-02-16 17:09:48 - Limiting validation dataset to:  103 images (281 boxes)\n",
      "2024-02-16 17:09:48 - Limiting test dataset to:  350 images (1272 boxes)\n",
      "\n",
      "-------------------------------------\n",
      " 'train' set statistics\n",
      "-------------------------------------\n",
      "  Image count:  2046\n",
      "  Bounding box count:  10602\n",
      "  Bounding box distribution: \n",
      "    Strawberry:  4276/10602 = 0.40\n",
      "    Orange:  3544/10602 = 0.33\n",
      "    Apple:  1849/10602 = 0.17\n",
      "    Banana:  933/10602 = 0.09\n",
      " \n",
      "\n",
      "-------------------------------------\n",
      " 'validation' set statistics\n",
      "-------------------------------------\n",
      "  Image count:  103\n",
      "  Bounding box count:  281\n",
      "  Bounding box distribution: \n",
      "    Strawberry:  144/281 = 0.51\n",
      "    Orange:  85/281 = 0.30\n",
      "    Apple:  42/281 = 0.15\n",
      "    Banana:  10/281 = 0.04\n",
      " \n",
      "\n",
      "-------------------------------------\n",
      " 'test' set statistics\n",
      "-------------------------------------\n",
      "  Image count:  350\n",
      "  Bounding box count:  1272\n",
      "  Bounding box distribution: \n",
      "    Orange:  551/1272 = 0.43\n",
      "    Strawberry:  421/1272 = 0.33\n",
      "    Apple:  210/1272 = 0.17\n",
      "    Banana:  90/1272 = 0.07\n",
      " \n",
      "\n",
      "-------------------------------------\n",
      " Overall statistics\n",
      "-------------------------------------\n",
      "  Image count:  2499\n",
      "  Bounding box count:  12155\n",
      "\n",
      "2024-02-16 17:09:48 - Saving 'train' data to /nvdli-nano/data/objectdetection/images/fruit/sub-train-annotations-bbox.csv.\n",
      "2024-02-16 17:09:49 - Saving 'validation' data to /nvdli-nano/data/objectdetection/images/fruit/sub-validation-annotations-bbox.csv.\n",
      "2024-02-16 17:09:49 - Saving 'test' data to /nvdli-nano/data/objectdetection/images/fruit/sub-test-annotations-bbox.csv.\n",
      "2024-02-16 17:09:49 - Starting to download 2499 images.\n",
      "2024-02-16 17:09:54 - Downloaded 100 images.\n",
      "2024-02-16 17:09:58 - Downloaded 200 images.\n",
      "2024-02-16 17:10:02 - Downloaded 300 images.\n",
      "2024-02-16 17:10:05 - Downloaded 400 images.\n",
      "2024-02-16 17:10:10 - Downloaded 500 images.\n",
      "2024-02-16 17:10:13 - Downloaded 600 images.\n",
      "2024-02-16 17:10:17 - Downloaded 700 images.\n",
      "2024-02-16 17:10:20 - Downloaded 800 images.\n",
      "2024-02-16 17:10:24 - Downloaded 900 images.\n",
      "2024-02-16 17:10:28 - Downloaded 1000 images.\n",
      "2024-02-16 17:10:31 - Downloaded 1100 images.\n",
      "2024-02-16 17:10:35 - Downloaded 1200 images.\n",
      "2024-02-16 17:10:38 - Downloaded 1300 images.\n",
      "2024-02-16 17:10:41 - Downloaded 1400 images.\n",
      "2024-02-16 17:10:47 - Downloaded 1500 images.\n",
      "2024-02-16 17:10:51 - Downloaded 1600 images.\n",
      "2024-02-16 17:10:55 - Downloaded 1700 images.\n",
      "2024-02-16 17:10:59 - Downloaded 1800 images.\n",
      "2024-02-16 17:11:03 - Downloaded 1900 images.\n",
      "2024-02-16 17:11:06 - Downloaded 2000 images.\n",
      "2024-02-16 17:11:09 - Downloaded 2100 images.\n",
      "2024-02-16 17:11:13 - Downloaded 2200 images.\n",
      "2024-02-16 17:11:17 - Downloaded 2300 images.\n",
      "2024-02-16 17:11:23 - Downloaded 2400 images.\n",
      "2024-02-16 17:11:42 - Task Done.\n"
     ]
    }
   ],
   "source": [
    "!python3 ../jetson-inference/python/training/detection/ssd/open_images_downloader.py --max-image=2500 --class-names \"Apple,Orange,Banana,Strawberry\" --data={DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00982a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n",
    "    net.train(True)\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_regression_loss = 0.0\n",
    "    train_classification_loss = 0.0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    \n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        confidence, locations = net(images)\n",
    "        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n",
    "        loss = regression_loss + classification_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_regression_loss += regression_loss.item()\n",
    "        train_classification_loss += classification_loss.item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "\n",
    "        if i and i % debug_steps == 0:\n",
    "            avg_loss = running_loss / debug_steps\n",
    "            avg_reg_loss = running_regression_loss / debug_steps\n",
    "            avg_clf_loss = running_classification_loss / debug_steps\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, Step: {i}/{len(loader)}, \" +\n",
    "                f\"Avg Loss: {avg_loss:.4f}, \" +\n",
    "                f\"Avg Regression Loss {avg_reg_loss:.4f}, \" +\n",
    "                f\"Avg Classification Loss: {avg_clf_loss:.4f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            running_regression_loss = 0.0\n",
    "            running_classification_loss = 0.0\n",
    "\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss /= num_batches\n",
    "    train_regression_loss /= num_batches\n",
    "    train_classification_loss /= num_batches\n",
    "    \n",
    "    logging.info(\n",
    "        f\"Epoch: {epoch}, \" +\n",
    "        f\"Training Loss: {train_loss:.4f}, \" +\n",
    "        f\"Training Regression Loss {train_regression_loss:.4f}, \" +\n",
    "        f\"Training Classification Loss: {train_classification_loss:.4f}\"\n",
    "    )\n",
    "     \n",
    "    tensorboard.add_scalar('Loss/train', train_loss, epoch)\n",
    "    tensorboard.add_scalar('Regression Loss/train', train_regression_loss, epoch)\n",
    "    tensorboard.add_scalar('Classification Loss/train', train_classification_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21180f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, net, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    num = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        num += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            confidence, locations = net(images)\n",
    "            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n",
    "            loss = regression_loss + classification_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "    \n",
    "    return running_loss / num, running_regression_loss / num, running_classification_loss / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c03494c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_net = create_mobilenetv1_ssd\n",
    "config = mobilenetv1_ssd_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89550037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = TrainAugmentation(config.image_size, config.image_mean, config.image_std)\n",
    "target_transform = MatchPrior(config.priors, config.center_variance,\n",
    "                              config.size_variance, 0.5)\n",
    "test_transform = TestTransform(config.image_size, config.image_mean, config.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "240900ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:29:17 - Prepare training datasets.\n",
      "2024-02-16 17:29:17 - loading annotations from: /nvdli-nano/data/objectdetection/images/fruit/sub-train-annotations-bbox.csv\n",
      "2024-02-16 17:29:17 - annotations loaded from:  /nvdli-nano/data/objectdetection/images/fruit/sub-train-annotations-bbox.csv\n",
      "num images:  2046\n",
      "2024-02-16 17:29:23 - balancing data\n",
      "2024-02-16 17:29:23 - Dataset Summary:Number of Images: 1616\n",
      "Minimum Number of Images for a Class: 412\n",
      "Label Distribution:\n",
      "\tApple: 1596\n",
      "\tBanana: 933\n",
      "\tOrange: 2949\n",
      "\tStrawberry: 2726\n",
      "2024-02-16 17:29:23 - Stored labels into file ../jetson-inference/python/training/detection/ssd/models/labels.txt.\n",
      "2024-02-16 17:29:23 - Train dataset size: 1616\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Prepare training datasets.\")\n",
    "datasets = []\n",
    "dataset = OpenImagesDataset(DATA_DIR, transform=train_transform, target_transform=target_transform,dataset_type=\"train\", balance_data=True)\n",
    "label_file = os.path.join(CHECK_POINT, \"labels.txt\")\n",
    "store_labels(label_file, dataset.class_names)\n",
    "logging.info(dataset)\n",
    "num_classes = len(dataset.class_names)       \n",
    "datasets.append(dataset)\n",
    "\n",
    "# create training dataset\n",
    "logging.info(f\"Stored labels into file {label_file}.\")\n",
    "train_dataset = ConcatDataset(datasets)\n",
    "logging.info(\"Train dataset size: {}\".format(len(train_dataset)))\n",
    "train_loader = DataLoader(train_dataset, BATCH, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3a74b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:34:58 - Prepare Validation datasets.\n",
      "2024-02-16 17:34:58 - loading annotations from: /nvdli-nano/data/objectdetection/images/fruit/sub-test-annotations-bbox.csv\n",
      "2024-02-16 17:34:58 - annotations loaded from:  /nvdli-nano/data/objectdetection/images/fruit/sub-test-annotations-bbox.csv\n",
      "num images:  350\n",
      "2024-02-16 17:34:59 - Dataset Summary:Number of Images: 350\n",
      "Minimum Number of Images for a Class: -1\n",
      "Label Distribution:\n",
      "\tApple: 210\n",
      "\tBanana: 90\n",
      "\tOrange: 551\n",
      "\tStrawberry: 421\n",
      "2024-02-16 17:34:59 - Validation dataset size: 350\n",
      "2024-02-16 17:34:59 - Build network.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Prepare Validation datasets.\")\n",
    "val_dataset = OpenImagesDataset(DATA_DIR, transform=test_transform, target_transform=target_transform, dataset_type=\"test\")\n",
    "logging.info(val_dataset)\n",
    "logging.info(\"Validation dataset size: {}\".format(len(val_dataset)))\n",
    "val_loader = DataLoader(val_dataset, BATCH,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        shuffle=False)\n",
    "\n",
    "# create the network\n",
    "logging.info(\"Build network.\")\n",
    "net = create_net(num_classes)\n",
    "min_loss = -10000.0\n",
    "last_epoch = -1\n",
    "\n",
    "params = [\n",
    "            {'params': net.base_net.parameters(), 'lr': base_net_lr},\n",
    "            {'params': itertools.chain(\n",
    "                net.source_layer_add_ons.parameters(),\n",
    "                net.extras.parameters()\n",
    "            ), 'lr': extra_layers_lr},\n",
    "            {'params': itertools.chain(\n",
    "                net.regression_headers.parameters(),\n",
    "                net.classification_headers.parameters()\n",
    "            )}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfe62db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:35:02 - Init from pretrained SSD\n",
      "2024-02-16 17:35:02 - Took 0.15 seconds to load the model.\n",
      "2024-02-16 17:35:02 - Learning rate: 0.01, Base net learning rate: 0.001, Extra Layers learning rate: None.\n",
      "2024-02-16 17:35:02 - Uses CosineAnnealingLR scheduler.\n",
      "2024-02-16 17:35:02 - Start training from epoch 0.\n"
     ]
    }
   ],
   "source": [
    "timer.start(\"Load Model\")\n",
    "base_net_lr = BASE_NET_LR\n",
    "extra_layers_lr = None\n",
    "logging.info(f\"Init from pretrained SSD\")\n",
    "if not os.path.exists(DEFAULT_PRETRAINED_MODEL):\n",
    "    os.system(f\"wget --quiet --show-progress --progress=bar:force:noscroll --no-check-certificate https://nvidia.box.com/shared/static/djf5w54rjvpqocsiztzaandq1m3avr7c.pth -O {DEFAULT_PRETRAINED_MODEL}\")\n",
    "net.init_from_pretrained_ssd(DEFAULT_PRETRAINED_MODEL)\n",
    "\n",
    "logging.info(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n",
    "net.to(DEVICE)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = MultiboxLoss(config.priors, iou_threshold=0.5, neg_pos_ratio=3,\n",
    "                         center_variance=0.1, size_variance=0.2, device=DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=LR, momentum=MOMENTUM,\n",
    "                            weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "logging.info(f\"Learning rate: {LR}, Base net learning rate: {base_net_lr}, \"\n",
    "             + f\"Extra Layers learning rate: {extra_layers_lr}.\")\n",
    "logging.info(\"Uses CosineAnnealingLR scheduler.\")\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_MAX, last_epoch=last_epoch)\n",
    "                        \n",
    "logging.info(f\"Start training from epoch {last_epoch + 1}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "003337da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-88743617a31d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEBUG_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mVALIDATION_EPOCHS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-51ddc7fdb688>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, net, criterion, optimizer, device, debug_steps, epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclassification_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov=nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'NoneType'"
     ]
    }
   ],
   "source": [
    "for epoch in range(last_epoch + 1, NUM_EPOCHS):\n",
    "        train(train_loader, net, criterion, optimizer, device=DEVICE, debug_steps=DEBUG_STEPS, epoch=epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % VALIDATION_EPOCHS == 0 or epoch == NUM_EPOCHS - 1:\n",
    "            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n",
    "            \n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, \" +\n",
    "                f\"Validation Loss: {val_loss:.4f}, \" +\n",
    "                f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n",
    "                f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n",
    "            )\n",
    "                    \n",
    "            tensorboard.add_scalar('Loss/val', val_loss, epoch)\n",
    "            tensorboard.add_scalar('Regression Loss/val', val_regression_loss, epoch)\n",
    "            tensorboard.add_scalar('Classification Loss/val', val_classification_loss, epoch)\n",
    "    \n",
    "            \n",
    "            mean_ap, class_ap = eval.compute()\n",
    "            eval.log_results(mean_ap, class_ap, f\"Epoch: {epoch}, \")\n",
    "\n",
    "            tensorboard.add_scalar('Mean Average Precision/val', mean_ap, epoch)\n",
    "\n",
    "            for i in range(len(class_ap)):\n",
    "                tensorboard.add_scalar(f\"Class Average Precision/{eval_dataset.class_names[i+1]}\", class_ap[i], epoch)\n",
    "    \n",
    "            model_path = os.path.join(CHECK_POINT, f\"MOBILE-SSD1-Epoch-{epoch}-Loss-{val_loss}.pth\")\n",
    "            net.save(model_path)\n",
    "            logging.info(f\"Saved model {model_path}\")\n",
    "\n",
    "logging.info(\"Task done, exiting program.\")\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c99c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
